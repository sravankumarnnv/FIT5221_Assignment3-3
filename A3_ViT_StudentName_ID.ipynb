{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9296,
     "status": "ok",
     "timestamp": 1746707147685,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "ASKQ6b0VrsuE",
    "outputId": "9f123e31-1901-40f5-b917-e326c659357c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19340,
     "status": "ok",
     "timestamp": 1746707181836,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "UYl2KI3KkZSS",
    "outputId": "9f2da9bc-ec8b-4e46-8d8b-a27f65e80b11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from timm.models.layers import DropPath, Mlp, PatchEmbed\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import OxfordIIITPet, wrap_dataset_for_transforms_v2\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(42) # For reproduciblity purpose, please do not modify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKFogPn5sNcv"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4All6iA9sPJu"
   },
   "outputs": [],
   "source": [
    "# You can change input params of this function if needed\n",
    "def load_pet_data(data_dir, batch_size=64, img_height=224, img_width=224, transform=None):\n",
    "\n",
    "    # Define transforms\n",
    "    if transform is None:\n",
    "      transform = v2.Compose([\n",
    "              v2.ToImage(),\n",
    "              v2.Resize((img_height, img_width), interpolation=InterpolationMode.BICUBIC),\n",
    "              v2.ToDtype(torch.float32, scale=True),\n",
    "          ])\n",
    "\n",
    "    train_ds = OxfordIIITPet(\n",
    "        root=data_dir,\n",
    "        split=\"trainval\",\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    val_ds = OxfordIIITPet(\n",
    "        root=data_dir,\n",
    "        split=\"test\",\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_ds)}\")\n",
    "    print(f\"Number of validation samples: {len(val_ds)}\")\n",
    "\n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, 37\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5PeAkE-mNAY"
   },
   "source": [
    "# Vision Transformer (ViT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOVB3kP21vZ2"
   },
   "outputs": [],
   "source": [
    "def swish(x, beta):\n",
    "    return x * torch.sigmoid(x * beta)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        attention = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.attn_drop(attention)\n",
    "\n",
    "        x = (attention @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attention = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.ffn = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attention(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        distilled=False,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.2,\n",
    "        embed_layer=PatchEmbed,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = (\n",
    "            embed_dim  # num_features for consistency with other models\n",
    "        )\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = nn.LayerNorm\n",
    "        act_layer = nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = (\n",
    "            nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        )\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + self.num_tokens, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                ViTLayer(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=norm_layer,\n",
    "                    act_layer=act_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, 1000)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat(\n",
    "                (cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1\n",
    "            )\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = x[:, self.num_tokens :].mean(dim=1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dNhU7AfzghW"
   },
   "source": [
    "# Task 3: Fine-tuning (a part of) the ViT Model (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1746707210989,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "KfahymMW3aEJ",
    "outputId": "c67c1516-cd68-4675-f9ae-faf8df01abb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: []\n",
      "Unexpected: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.018)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.036)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.055)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.073)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.091)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.109)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.127)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.145)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.164)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.182)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ViTLayer(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attention): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.200)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=384, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 # Set this to your preferred values.\n",
    "\n",
    "small_vit = VisionTransformer()\n",
    "\n",
    "# Load the pretrained ImageNet weight\n",
    "missing, unexpected = small_vit.load_state_dict(torch.load(\"./pretrained_vit_small.pth\"))\n",
    "\n",
    "print(\"Missing:\", missing)\n",
    "print(\"Unexpected:\", unexpected)\n",
    "\n",
    "# Replace with a new head for transfer learning.\n",
    "# Hidden dimension of 384, 37 classes for the OxfordIIITPet dataset\n",
    "small_vit.head = nn.Linear(384, 37)\n",
    "small_vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6592,
     "status": "ok",
     "timestamp": 1746707220645,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "XHdT9pOH8iAS",
    "outputId": "a53d34c3-54b9-43e4-df55-f1f3b284c2f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [32, 37]                  76,032\n",
       "├─PatchEmbed: 1-1                        [32, 196, 384]            --\n",
       "│    └─Conv2d: 2-1                       [32, 384, 14, 14]         295,296\n",
       "│    └─Identity: 2-2                     [32, 196, 384]            --\n",
       "├─Dropout: 1-2                           [32, 197, 384]            --\n",
       "├─Sequential: 1-3                        [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-3                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-1               [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-2               [32, 197, 384]            591,360\n",
       "│    │    └─Identity: 3-3                [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-4               [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-5                     [32, 197, 384]            1,181,568\n",
       "│    │    └─Identity: 3-6                [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-4                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-7               [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-8               [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-9                [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-10              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-11                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-12               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-5                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-13              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-14              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-15               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-16              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-17                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-18               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-6                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-19              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-20              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-21               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-22              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-23                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-24               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-7                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-25              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-26              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-27               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-28              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-29                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-30               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-8                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-31              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-32              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-33               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-34              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-35                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-36               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-9                     [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-37              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-38              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-39               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-40              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-41                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-42               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-10                    [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-43              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-44              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-45               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-46              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-47                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-48               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-11                    [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-49              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-50              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-51               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-52              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-53                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-54               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-12                    [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-55              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-56              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-57               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-58              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-59                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-60               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-13                    [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-61              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-62              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-63               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-64              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-65                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-66               [32, 197, 384]            --\n",
       "│    └─ViTLayer: 2-14                    [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-67              [32, 197, 384]            768\n",
       "│    │    └─Attention: 3-68              [32, 197, 384]            591,360\n",
       "│    │    └─DropPath: 3-69               [32, 197, 384]            --\n",
       "│    │    └─LayerNorm: 3-70              [32, 197, 384]            768\n",
       "│    │    └─Mlp: 3-71                    [32, 197, 384]            1,181,568\n",
       "│    │    └─DropPath: 3-72               [32, 197, 384]            --\n",
       "├─LayerNorm: 1-4                         [32, 384]                 768\n",
       "├─Linear: 1-5                            [32, 37]                  14,245\n",
       "==========================================================================================\n",
       "Total params: 21,679,909\n",
       "Trainable params: 21,679,909\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.53\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 2575.67\n",
       "Params size (MB): 86.42\n",
       "Estimated Total Size (MB): 2681.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(small_vit, (batch_size, 3, 224, 224), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66366,
     "status": "ok",
     "timestamp": 1746707309799,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "hvtPU5If0xQv",
    "outputId": "9ebaa7d7-2859-4030-fd0a-42763e4f1338"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 792M/792M [00:43<00:00, 18.3MB/s]\n",
      "100%|██████████| 19.2M/19.2M [00:01<00:00, 11.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3680\n",
      "Number of validation samples: 3669\n"
     ]
    }
   ],
   "source": [
    "# Get the dataloaders for training\n",
    "train_dataloader, val_dataloader, num_classes = load_pet_data(data_dir=\"./\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJc-PoDj2wmT"
   },
   "outputs": [],
   "source": [
    "# Your code starts from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IG8qCPBp0bLl"
   },
   "source": [
    "# Task 4: Explore and Implement a Parameter-Efficient Transfer Learning (PETL) Technique (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IXqmkRe0os0"
   },
   "outputs": [],
   "source": [
    "# Your code starts from here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1u7VyvmdEQtVPs_PJCA8jebUfleqHk5PK",
     "timestamp": 1746710720988
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
