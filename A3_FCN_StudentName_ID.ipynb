{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g52W_7ZaJvNX"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import torch\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(42) # For reproduciblity purpose, please do not modify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUD4udHlOJFR"
   },
   "source": [
    "## Helper functions and dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HOzyHbcXqvO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.dataset[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def target_transform(target):\n",
    "    target = target.resize((224, 224), Image.NEAREST)\n",
    "    target = torch.tensor(np.array(target), dtype=torch.long)\n",
    "    # Convert 255 (void) to 0 (background)\n",
    "    target[target == 255] = 0\n",
    "    return target\n",
    "\n",
    "# Create datasets\n",
    "train_voc = VOCDataset(train_dataset, transform=train_transform, target_transform=target_transform)\n",
    "val_voc = VOCDataset(val_dataset, transform=val_transform, target_transform=target_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_voc, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_voc, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_voc)}\")\n",
    "print(f\"Val samples: {len(val_voc)}\")\n",
    "print(f\"Number of classes: {len(VOC_CLASSES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJH919aoOOZT"
   },
   "source": [
    "##1. Download dataset\n",
    "Please refer to [this function](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.VOCSegmentation.html) from TorchVision to download the Pascal VOC Segmentation Dataset.\n",
    "\n",
    "Note that you can change the input of provided code to match with your requirement.\n",
    "\n",
    "Because the Pascal VOC Segmentation Dataset 2012 only provide a `train` set and a `val` set. So that you are required to train on `train` set only and then test the model on `val` set\n",
    "\n",
    "**Note:** There is a void class with index 255 in dataset, you can treat the pixels with this label as backbround or just simply ignore it when calculate the loss value. [Refer to this post for suggestion](https://discuss.pytorch.org/t/having-trouble-with-voc-2012-segmentation-with-the-void-255-label/46486/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87776,
     "status": "ok",
     "timestamp": 1746677701851,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "ai23DKZ2NXTa",
    "outputId": "20e47983-d946-4f29-c663-1432456af3d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00G/2.00G [00:51<00:00, 38.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "voc_dir = './data'\n",
    "os.makedirs(voc_dir, exist_ok=True)\n",
    "train_dataset = VOCSegmentation(root=voc_dir, year=\"2012\", image_set=\"train\", download=True)\n",
    "val_dataset = VOCSegmentation(root=voc_dir, year=\"2012\", image_set=\"val\", download=True)\n",
    "\n",
    "VOC_CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "               \"bus\", \"car\",  \"cat\",  \"chair\", \"cow\",  \"diningtable\", \"dog\", \"horse\",\n",
    "               \"motorbike\", \"person\",\"potted plant\", \"sheep\", \"sofa\",\"train\", \"tv/monitor\"]\n",
    "\n",
    "VOC_COLORMAP = [\n",
    "    [0, 0, 0],\n",
    "    [128, 0, 0],\n",
    "    [0, 128, 0],\n",
    "    [128, 128, 0],\n",
    "    [0, 0, 128],\n",
    "    [128, 0, 128],\n",
    "    [0, 128, 128],\n",
    "    [128, 128, 128],\n",
    "    [64, 0, 0],\n",
    "    [192, 0, 0],\n",
    "    [64, 128, 0],\n",
    "    [192, 128, 0],\n",
    "    [64, 0, 128],\n",
    "    [192, 0, 128],\n",
    "    [64, 128, 128],\n",
    "    [192, 128, 128],\n",
    "    [0, 64, 0],\n",
    "    [128, 64, 0],\n",
    "    [0, 192, 0],\n",
    "    [128, 192, 0],\n",
    "    [0, 64, 128],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M846w5vQgJj"
   },
   "source": [
    "##2. Helper function\n",
    "You are required to use this helper function to calculate the mean IoU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhJ57aMFKCjp"
   },
   "outputs": [],
   "source": [
    "# Provided meanIoU score\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_segmentation_metrics(preds, masks, num_classes, ignore_index=0):\n",
    "    \"\"\"\n",
    "    Computes segmentation metrics: per-class and mean Precision, Recall, IoU, Dice, and overall Pixel Accuracy.\n",
    "\n",
    "    Args:\n",
    "        preds (Tensor): Predicted segmentation masks (B, H, W), each element is the predicted index class\n",
    "        masks (Tensor): Ground truth segmentation masks (B, H, W)\n",
    "        num_classes (int): Number of classes including background\n",
    "        ignore_index (int): Label to ignore in evaluation (e.g., it should be the index of the background)\n",
    "\n",
    "    Returns:\n",
    "        metrics (dict): Dictionary containing:\n",
    "            - 'per_class': dict of per-class metrics\n",
    "            - 'mean_metrics': dict of averaged metrics across foreground classes\n",
    "            - 'pixel_accuracy': float, overall pixel accuracy (excluding ignored)\n",
    "    \"\"\"\n",
    "    eps = 1e-6  # for numerical stability\n",
    "    preds = preds.view(-1)\n",
    "    masks = masks.view(-1)\n",
    "    valid = masks != ignore_index\n",
    "\n",
    "    preds = preds[valid]\n",
    "    masks = masks[valid]\n",
    "\n",
    "    per_class_metrics = {}\n",
    "    total_correct = 0\n",
    "    total_pixels = valid.sum().item()\n",
    "\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    iou_list = []\n",
    "    dice_list = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = preds == cls\n",
    "        target_inds = masks == cls\n",
    "\n",
    "        TP = (pred_inds & target_inds).sum().item()\n",
    "        FP = (pred_inds & ~target_inds).sum().item()\n",
    "        FN = (~pred_inds & target_inds).sum().item()\n",
    "        TN = ((~pred_inds) & (~target_inds)).sum().item()\n",
    "\n",
    "        union = TP + FP + FN\n",
    "        pred_sum = pred_inds.sum().item()\n",
    "        target_sum = target_inds.sum().item()\n",
    "\n",
    "        if target_sum == 0 and pred_sum == 0:\n",
    "            continue\n",
    "\n",
    "        precision = TP / (TP + FP + eps)\n",
    "        recall = TP / (TP + FN + eps)\n",
    "        iou = TP / (union + eps)\n",
    "        dice = (2 * TP) / (pred_sum + target_sum + eps)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        iou_list.append(iou)\n",
    "        dice_list.append(dice)\n",
    "\n",
    "        total_correct += TP\n",
    "\n",
    "    pixel_accuracy = total_correct / (total_pixels + eps)\n",
    "\n",
    "    return {\n",
    "        \"precision\": sum(precision_list) / len(precision_list),\n",
    "        \"recall\": sum(recall_list) / len(recall_list),\n",
    "        \"iou\": sum(iou_list) / len(iou_list),\n",
    "        \"dice\": sum(dice_list) / len(dice_list),\n",
    "        \"pixel_accuracy\": pixel_accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1HxCIAxQrz4"
   },
   "source": [
    "# Task 1: Build a baseline Fully Convolutional Network (FCN) model for semantic segmentation (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9034,
     "status": "ok",
     "timestamp": 1746677842685,
     "user": {
      "displayName": "Tuan Huynh",
      "userId": "13723204249573107676"
     },
     "user_tz": -600
    },
    "id": "LqJtgLwvRW9S",
    "outputId": "ba0ff4de-a21a-4598-c9c1-ed92d6f4c5a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n",
      "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/efficientnet_b0_pyt_amp/versions/20.12.0/files/nvidia_efficientnet-b0_210412.pth\" to /root/.cache/torch/hub/checkpoints/nvidia_efficientnet-b0_210412.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 128MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Note: You can modify this code to load the backbone, just make sure you use model and weights from Nvidia\n",
    "backbone_efficientnet = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\",  \"nvidia_efficientnet_b0\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHMnvR4EEEmg"
   },
   "outputs": [],
   "source": [
    "# Your code starts from here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming backbone_efficientnet and device are already defined\n",
    "# Also assuming VOC_CLASSES is defined and contains the class names\n",
    "\n",
    "# Baseline FCN Model\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=21):\n",
    "        super(FCN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Remove the classifier and avgpool layers\n",
    "        self.features = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        \n",
    "        # Get feature map size by doing a forward pass\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.randn(1, 3, 224, 224)\n",
    "            feature_map = self.features(sample_input)\n",
    "            self.feature_channels = feature_map.shape[1]\n",
    "            self.feature_size = feature_map.shape[2:]\n",
    "        \n",
    "        # FCN classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(self.feature_channels, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Upsampling layer to restore original image size\n",
    "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.features(x)\n",
    "        \n",
    "        # Apply classifier\n",
    "        out = self.classifier(features)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        out = self.upsample(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create the baseline FCN model\n",
    "model = FCN(backbone_efficientnet, num_classes=len(VOC_CLASSES))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore background class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        total_samples += images.size(0)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return running_loss / total_samples\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_segmentation_metrics(all_preds, all_targets, len(VOC_CLASSES), ignore_index=0)\n",
    "    \n",
    "    return running_loss / total_samples, metrics\n",
    "\n",
    "# Training loop for baseline model\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ious = []\n",
    "\n",
    "print(\"Training Baseline FCN Model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_ious.append(val_metrics['iou'])\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val IoU: {val_metrics['iou']:.4f}\")\n",
    "    print(f\"Val Pixel Accuracy: {val_metrics['pixel_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nBaseline FCN Training Complete!\")\n",
    "print(f\"Best Validation IoU: {max(val_ious):.4f}\")\n",
    "\n",
    "# Save the baseline model\n",
    "torch.save(model.state_dict(), 'baseline_fcn_model.pth')\n",
    "print(\"Baseline model saved as 'baseline_fcn_model.pth'\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_ious, label='Val IoU', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU Score')\n",
    "plt.title('Validation IoU Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXgFt_ZFQy-e"
   },
   "source": [
    "# Task 2: Improve the baseline FCN model (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yam5HEqqEFp8"
   },
   "outputs": [],
   "source": [
    "# Improved FCN Model with Skip Connections and Advanced Architecture\n",
    "class ImprovedFCN(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=21):\n",
    "        super(ImprovedFCN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Extract features at multiple scales for skip connections\n",
    "        self.layer1 = nn.Sequential(*list(backbone.children())[:3])  # Early features\n",
    "        self.layer2 = nn.Sequential(*list(backbone.children())[3:5])  # Mid features  \n",
    "        self.layer3 = nn.Sequential(*list(backbone.children())[5:7])  # High-level features\n",
    "        self.layer4 = nn.Sequential(*list(backbone.children())[7:-2])  # Final features\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.randn(1, 3, 224, 224)\n",
    "            f1 = self.layer1(sample_input)\n",
    "            f2 = self.layer2(f1)\n",
    "            f3 = self.layer3(f2)\n",
    "            f4 = self.layer4(f3)\n",
    "            \n",
    "            self.f1_channels = f1.shape[1]\n",
    "            self.f2_channels = f2.shape[1] \n",
    "            self.f3_channels = f3.shape[1]\n",
    "            self.f4_channels = f4.shape[1]\n",
    "        \n",
    "        # Reduce channel dimensions for skip connections\n",
    "        self.skip1 = nn.Conv2d(self.f1_channels, 64, 1)\n",
    "        self.skip2 = nn.Conv2d(self.f2_channels, 128, 1)\n",
    "        self.skip3 = nn.Conv2d(self.f3_channels, 256, 1)\n",
    "        \n",
    "        # Main classifier path\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(self.f4_channels, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Fusion layers for combining features\n",
    "        self.fusion3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.fusion2 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.fusion1 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, 1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(64, 16, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Multi-scale feature extraction\n",
    "        f1 = self.layer1(x)  # 1/4 scale\n",
    "        f2 = self.layer2(f1)  # 1/8 scale\n",
    "        f3 = self.layer3(f2)  # 1/16 scale\n",
    "        f4 = self.layer4(f3)  # 1/32 scale\n",
    "        \n",
    "        # Main classification path\n",
    "        out = self.classifier(f4)\n",
    "        \n",
    "        # Progressive upsampling with skip connections\n",
    "        # Stage 1: Upsample and fuse with f3\n",
    "        out = F.interpolate(out, size=f3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        skip3 = self.skip3(f3)\n",
    "        out = torch.cat([out, skip3], dim=1)\n",
    "        out = self.fusion3(out)\n",
    "        \n",
    "        # Stage 2: Upsample and fuse with f2\n",
    "        out = F.interpolate(out, size=f2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        skip2 = self.skip2(f2)\n",
    "        out = torch.cat([out, skip2], dim=1)\n",
    "        out = self.fusion2(out)\n",
    "        \n",
    "        # Stage 3: Upsample and fuse with f1\n",
    "        out = F.interpolate(out, size=f1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        skip1 = self.skip1(f1)\n",
    "        out = torch.cat([out, skip1], dim=1)\n",
    "        out = self.fusion1(out)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_map = self.attention(out)\n",
    "        out = out * attention_map\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.final_conv(out)\n",
    "        \n",
    "        # Upsample to original size\n",
    "        out = F.interpolate(out, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create improved model\n",
    "improved_model = ImprovedFCN(backbone_efficientnet, num_classes=len(VOC_CLASSES))\n",
    "improved_model = improved_model.to(device)\n",
    "\n",
    "# Advanced loss function - Focal Loss for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, ignore_index=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, ignore_index=self.ignore_index, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=0):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.focal_loss = FocalLoss(ignore_index=ignore_index)\n",
    "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        focal = self.focal_loss(inputs, targets)\n",
    "        ce = self.ce_loss(inputs, targets)\n",
    "        return 0.7 * focal + 0.3 * ce\n",
    "\n",
    "# Improved training setup\n",
    "criterion_improved = CombinedLoss(ignore_index=0)\n",
    "optimizer_improved = optim.AdamW(improved_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler_improved = optim.lr_scheduler.CosineAnnealingLR(optimizer_improved, T_max=20, eta_min=1e-6)\n",
    "\n",
    "print(f\"Improved model created with {sum(p.numel() for p in improved_model.parameters())} parameters\")\n",
    "\n",
    "# Enhanced data augmentation for improved model\n",
    "train_transform_improved = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def target_transform_improved(target):\n",
    "    target = target.resize((256, 256), Image.NEAREST)\n",
    "    target = transforms.CenterCrop((224, 224))(target)\n",
    "    target = torch.tensor(np.array(target), dtype=torch.long)\n",
    "    target[target == 255] = 0\n",
    "    return target\n",
    "\n",
    "# Create improved datasets\n",
    "train_voc_improved = VOCDataset(train_dataset, transform=train_transform_improved, target_transform=target_transform_improved)\n",
    "val_voc_improved = VOCDataset(val_dataset, transform=val_transform, target_transform=target_transform)\n",
    "\n",
    "train_loader_improved = DataLoader(train_voc_improved, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader_improved = DataLoader(val_voc_improved, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Training loop for improved model\n",
    "num_epochs_improved = 20\n",
    "train_losses_improved = []\n",
    "val_losses_improved = []\n",
    "val_ious_improved = []\n",
    "\n",
    "print(\"\\nTraining Improved FCN Model...\")\n",
    "for epoch in range(num_epochs_improved):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs_improved}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(improved_model, train_loader_improved, criterion_improved, optimizer_improved, device)\n",
    "    \n",
    "    # Validate \n",
    "    val_loss, val_metrics = validate_epoch(improved_model, val_loader_improved, criterion_improved, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler_improved.step()\n",
    "    \n",
    "    train_losses_improved.append(train_loss)\n",
    "    val_losses_improved.append(val_loss)\n",
    "    val_ious_improved.append(val_metrics['iou'])\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val IoU: {val_metrics['iou']:.4f}\")\n",
    "    print(f\"Val Pixel Accuracy: {val_metrics['pixel_accuracy']:.4f}\")\n",
    "    print(f\"Val Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Val Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"Val Dice: {val_metrics['dice']:.4f}\")\n",
    "\n",
    "print(f\"\\nImproved FCN Training Complete!\")\n",
    "print(f\"Best Validation IoU: {max(val_ious_improved):.4f}\")\n",
    "print(f\"Improvement over baseline: {max(val_ious_improved) - max(val_ious):.4f}\")\n",
    "\n",
    "# Save the improved model\n",
    "torch.save(improved_model.state_dict(), 'improved_fcn_model.pth')\n",
    "print(\"Improved model saved as 'improved_fcn_model.pth'\")\n",
    "\n",
    "# Compare results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Baseline Train', alpha=0.7)\n",
    "plt.plot(val_losses, label='Baseline Val', alpha=0.7)\n",
    "plt.plot(train_losses_improved, label='Improved Train')\n",
    "plt.plot(val_losses_improved, label='Improved Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_ious, label='Baseline IoU', alpha=0.7)\n",
    "plt.plot(val_ious_improved, label='Improved IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU Score')\n",
    "plt.title('Validation IoU Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "epochs = range(1, len(val_ious) + 1)\n",
    "epochs_improved = range(1, len(val_ious_improved) + 1)\n",
    "plt.bar(['Baseline', 'Improved'], [max(val_ious), max(val_ious_improved)], \n",
    "        color=['lightblue', 'darkblue'], alpha=0.8)\n",
    "plt.ylabel('Best IoU Score')\n",
    "plt.title('Best Validation IoU Comparison')\n",
    "for i, v in enumerate([max(val_ious), max(val_ious_improved)]):\n",
    "    plt.text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample predictions\n",
    "def visualize_predictions(model, dataloader, num_samples=3):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples*3))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(dataloader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Get first image from batch\n",
    "            img = images[0].cpu()\n",
    "            target = targets[0].cpu()\n",
    "            pred = preds[0].cpu()\n",
    "            \n",
    "            # Denormalize image\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            img = img * std + mean\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "            \n",
    "            axes[i, 0].imshow(img.permute(1, 2, 0))\n",
    "            axes[i, 0].set_title('Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(target, cmap='tab20')\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(pred, cmap='tab20')\n",
    "            axes[i, 2].set_title('Prediction')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSample predictions from improved model:\")\n",
    "visualize_predictions(improved_model, val_loader_improved)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
