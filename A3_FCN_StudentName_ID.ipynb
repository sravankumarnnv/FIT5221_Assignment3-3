{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"g52W_7ZaJvNX"},"outputs":[],"source":["# import library\n","import os\n","import torch\n","from torchvision.datasets import VOCSegmentation\n","import random\n","import numpy as np\n","\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","set_seed(42) # For reproduciblity purpose, please do not modify this."]},{"cell_type":"markdown","source":["## Helper functions and dataset setup"],"metadata":{"id":"TUD4udHlOJFR"}},{"cell_type":"code","source":[],"metadata":{"id":"2HOzyHbcXqvO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##1. Download dataset\n","Please refer to [this function](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.VOCSegmentation.html) from TorchVision to download the Pascal VOC Segmentation Dataset.\n","\n","Note that you can change the input of provided code to match with your requirement.\n","\n","Because the Pascal VOC Segmentation Dataset 2012 only provide a `train` set and a `val` set. So that you are required to train on `train` set only and then test the model on `val` set\n","\n","**Note:** There is a void class with index 255 in dataset, you can treat the pixels with this label as backbround or just simply ignore it when calculate the loss value. [Refer to this post for suggestion](https://discuss.pytorch.org/t/having-trouble-with-voc-2012-segmentation-with-the-void-255-label/46486/7)"],"metadata":{"id":"cJH919aoOOZT"}},{"cell_type":"code","source":["voc_dir = './data'\n","os.makedirs(voc_dir, exist_ok=True)\n","train_dataset = VOCSegmentation(root=voc_dir, year=\"2012\", image_set=\"train\", download=True)\n","val_dataset = VOCSegmentation(root=voc_dir, year=\"2012\", image_set=\"val\", download=True)\n","\n","VOC_CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n","               \"bus\", \"car\",  \"cat\",  \"chair\", \"cow\",  \"diningtable\", \"dog\", \"horse\",\n","               \"motorbike\", \"person\",\"potted plant\", \"sheep\", \"sofa\",\"train\", \"tv/monitor\"]\n","\n","VOC_COLORMAP = [\n","    [0, 0, 0],\n","    [128, 0, 0],\n","    [0, 128, 0],\n","    [128, 128, 0],\n","    [0, 0, 128],\n","    [128, 0, 128],\n","    [0, 128, 128],\n","    [128, 128, 128],\n","    [64, 0, 0],\n","    [192, 0, 0],\n","    [64, 128, 0],\n","    [192, 128, 0],\n","    [64, 0, 128],\n","    [192, 0, 128],\n","    [64, 128, 128],\n","    [192, 128, 128],\n","    [0, 64, 0],\n","    [128, 64, 0],\n","    [0, 192, 0],\n","    [128, 192, 0],\n","    [0, 64, 128],\n","]"],"metadata":{"id":"ai23DKZ2NXTa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746677701851,"user_tz":-600,"elapsed":87776,"user":{"displayName":"Tuan Huynh","userId":"13723204249573107676"}},"outputId":"20e47983-d946-4f29-c663-1432456af3d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2.00G/2.00G [00:51<00:00, 38.6MB/s]\n"]}]},{"cell_type":"markdown","source":["##2. Helper function\n","You are required to use this helper function to calculate the mean IoU score"],"metadata":{"id":"-M846w5vQgJj"}},{"cell_type":"code","source":["# Provided meanIoU score\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","def calculate_segmentation_metrics(preds, masks, num_classes, ignore_index=0):\n","    \"\"\"\n","    Computes segmentation metrics: per-class and mean Precision, Recall, IoU, Dice, and overall Pixel Accuracy.\n","\n","    Args:\n","        preds (Tensor): Predicted segmentation masks (B, H, W), each element is the predicted index class\n","        masks (Tensor): Ground truth segmentation masks (B, H, W)\n","        num_classes (int): Number of classes including background\n","        ignore_index (int): Label to ignore in evaluation (e.g., it should be the index of the background)\n","\n","    Returns:\n","        metrics (dict): Dictionary containing:\n","            - 'per_class': dict of per-class metrics\n","            - 'mean_metrics': dict of averaged metrics across foreground classes\n","            - 'pixel_accuracy': float, overall pixel accuracy (excluding ignored)\n","    \"\"\"\n","    eps = 1e-6  # for numerical stability\n","    preds = preds.view(-1)\n","    masks = masks.view(-1)\n","    valid = masks != ignore_index\n","\n","    preds = preds[valid]\n","    masks = masks[valid]\n","\n","    per_class_metrics = {}\n","    total_correct = 0\n","    total_pixels = valid.sum().item()\n","\n","    precision_list = []\n","    recall_list = []\n","    iou_list = []\n","    dice_list = []\n","\n","    for cls in range(num_classes):\n","        pred_inds = preds == cls\n","        target_inds = masks == cls\n","\n","        TP = (pred_inds & target_inds).sum().item()\n","        FP = (pred_inds & ~target_inds).sum().item()\n","        FN = (~pred_inds & target_inds).sum().item()\n","        TN = ((~pred_inds) & (~target_inds)).sum().item()\n","\n","        union = TP + FP + FN\n","        pred_sum = pred_inds.sum().item()\n","        target_sum = target_inds.sum().item()\n","\n","        if target_sum == 0 and pred_sum == 0:\n","            continue\n","\n","        precision = TP / (TP + FP + eps)\n","        recall = TP / (TP + FN + eps)\n","        iou = TP / (union + eps)\n","        dice = (2 * TP) / (pred_sum + target_sum + eps)\n","\n","        precision_list.append(precision)\n","        recall_list.append(recall)\n","        iou_list.append(iou)\n","        dice_list.append(dice)\n","\n","        total_correct += TP\n","\n","    pixel_accuracy = total_correct / (total_pixels + eps)\n","\n","    return {\n","        \"precision\": sum(precision_list) / len(precision_list),\n","        \"recall\": sum(recall_list) / len(recall_list),\n","        \"iou\": sum(iou_list) / len(iou_list),\n","        \"dice\": sum(dice_list) / len(dice_list),\n","        \"pixel_accuracy\": pixel_accuracy,\n","    }"],"metadata":{"id":"BhJ57aMFKCjp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Task 1: Build a baseline Fully Convolutional Network (FCN) model for semantic segmentation (5 marks)"],"metadata":{"id":"-1HxCIAxQrz4"}},{"cell_type":"code","source":["# Note: You can modify this code to load the backbone, just make sure you use model and weights from Nvidia\n","backbone_efficientnet = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\",  \"nvidia_efficientnet_b0\", pretrained=True)"],"metadata":{"id":"LqJtgLwvRW9S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746677842685,"user_tz":-600,"elapsed":9034,"user":{"displayName":"Tuan Huynh","userId":"13723204249573107676"}},"outputId":"ba0ff4de-a21a-4598-c9c1-ed92d6f4c5a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n","/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n","  warnings.warn(\n","/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n","  warnings.warn(\n","Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/efficientnet_b0_pyt_amp/versions/20.12.0/files/nvidia_efficientnet-b0_210412.pth\" to /root/.cache/torch/hub/checkpoints/nvidia_efficientnet-b0_210412.pth\n","100%|██████████| 20.5M/20.5M [00:00<00:00, 128MB/s] \n"]}]},{"cell_type":"code","source":["# Your code starts from here"],"metadata":{"id":"bHMnvR4EEEmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Task 2: Improve the baseline FCN model (8 marks)"],"metadata":{"id":"cXgFt_ZFQy-e"}},{"cell_type":"code","source":["# Your code starts from here"],"metadata":{"id":"yam5HEqqEFp8"},"execution_count":null,"outputs":[]}]}